{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d529c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"모든 라이브러리 시드 고정\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    set_seed(seed)\n",
    "\n",
    "# 시드 설정\n",
    "seed = 84\n",
    "set_all_seeds(seed)\n",
    "\n",
    "# 버전\n",
    "version = \"electra-kor-base-pt\"\n",
    "\n",
    "# 설정\n",
    "class Config:\n",
    "    \"\"\"학습에 필요한 하이퍼파라미터 및 설정을 관리합니다.\"\"\"\n",
    "    MODEL_NAME = \"kykim/electra-kor-base\"  # 한국어 모델\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EPOCHS = 3\n",
    "    WARMUP_RATIO = 0.1\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"디바이스: {cfg.DEVICE}\")\n",
    "print(f\"사용 모델: {cfg.MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train = pd.read_csv('./project/sentence/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d133f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성 함수 및 클래스 정의\n",
    "def create_enhanced_pairwise_data(df):\n",
    "   \"\"\"개선된 쌍 데이터 생성 - 다양한 거리의 positive/negative 샘플 포함\"\"\"\n",
    "   pairs = []\n",
    "   labels = []\n",
    "   seen_pairs = set()  # 중복 체크용 set\n",
    "\n",
    "   for _, row in tqdm(df.iterrows(), total=len(df), desc=\"개선된 쌍 데이터 생성 중\"):\n",
    "       sentences = [row[f'sentence_{i}'] for i in range(4)]\n",
    "       correct_order = [row[f'answer_{i}'] for i in range(4)]\n",
    "\n",
    "       # 1. 연속된 문장 쌍 (거리 1)\n",
    "       for i in range(3):\n",
    "           # Positive: 올바른 순서 (앞 문장 → 뒤 문장)\n",
    "           pairs.append((sentences[correct_order[i]], sentences[correct_order[i+1]]))\n",
    "           labels.append(1)\n",
    "\n",
    "           # Negative: 역순 (뒤 문장 → 앞 문장)\n",
    "           pairs.append((sentences[correct_order[i+1]], sentences[correct_order[i]]))\n",
    "           labels.append(0)\n",
    "\n",
    "       # 2. 비연속 문장 쌍 (거리 2)\n",
    "       for i in range(2):\n",
    "           # Positive: 올바른 순서\n",
    "           pairs.append((sentences[correct_order[i]], sentences[correct_order[i+2]]))\n",
    "           labels.append(1)\n",
    "\n",
    "           # Negative: 역순\n",
    "           pairs.append((sentences[correct_order[i+2]], sentences[correct_order[i]]))\n",
    "           labels.append(0)\n",
    "\n",
    "       # 3. 거리 3인 쌍 추가 (첫 번째와 마지막 문장)\n",
    "       pairs.append((sentences[correct_order[0]], sentences[correct_order[3]]))\n",
    "       labels.append(1)\n",
    "       pairs.append((sentences[correct_order[3]], sentences[correct_order[0]]))\n",
    "       labels.append(0)\n",
    "\n",
    "       # 4. 완전 역순 쌍 추가 (추가적인 hard negative)\n",
    "       reverse_order = correct_order[::-1]\n",
    "       for i in range(3):\n",
    "           pairs.append((sentences[reverse_order[i]], sentences[reverse_order[i+1]]))\n",
    "           labels.append(0)\n",
    "\n",
    "       # 5. 랜덤 negative 쌍 추가\n",
    "       all_indices = list(range(4))\n",
    "       negative_count = 0\n",
    "       max_attempts = 10\n",
    "\n",
    "       for attempt in range(max_attempts):\n",
    "           if negative_count >= 2:\n",
    "               break\n",
    "\n",
    "           idx1, idx2 = random.sample(all_indices, 2)\n",
    "           if correct_order.index(idx1) > correct_order.index(idx2):\n",
    "                pair_tuple = (sentences[idx1], sentences[idx2])\n",
    "                pair_hash = hash(pair_tuple)\n",
    "\n",
    "                if pair_hash not in seen_pairs:\n",
    "                    pairs.append(pair_tuple)\n",
    "                    labels.append(0)\n",
    "                    seen_pairs.add(pair_hash)\n",
    "                    negative_count += 1\n",
    "\n",
    "   return pairs, labels\n",
    "\n",
    "class SentenceOrderDataset(Dataset):\n",
    "    \"\"\"문장 순서 예측을 위한 PyTorch 커스텀 데이터셋\"\"\"\n",
    "    def __init__(self, pairs, labels, tokenizer, max_len):\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_A, sent_B = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sent_A, sent_B,\n",
    "            add_special_tokens=True, max_length=self.max_len,\n",
    "            padding='max_length', truncation=True,\n",
    "            return_token_type_ids=True, return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 학습 및 평가 함수 정의\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    \"\"\"1 에폭 동안 모델 학습\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_grad_norm = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"학습 진행 중\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids, labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        total_grad_norm += grad_norm.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_grad_norm = total_grad_norm / num_batches\n",
    "\n",
    "    return avg_loss, avg_grad_norm\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"모델 성능 평가\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 진행 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids, labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    return accuracy, total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "train_df, val_df = train_test_split(train, test_size=0.1, random_state=seed)\n",
    "\n",
    "# Pairwise 데이터 생성\n",
    "train_pairs, train_labels = create_enhanced_pairwise_data(train_df)\n",
    "val_pairs, val_labels = create_enhanced_pairwise_data(val_df)\n",
    "\n",
    "# 토크나이저 및 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cfg.MODEL_NAME, num_labels=2)\n",
    "model.to(cfg.DEVICE)\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "train_dataset = SentenceOrderDataset(train_pairs, train_labels, tokenizer, cfg.MAX_LENGTH)\n",
    "val_dataset = SentenceOrderDataset(val_pairs, val_labels, tokenizer, cfg.MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE)\n",
    "\n",
    "# 옵티마이저 및 스케줄러 설정\n",
    "optimizer = AdamW(model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "total_steps = len(train_loader) * cfg.EPOCHS\n",
    "warmup_steps = int(total_steps * cfg.WARMUP_RATIO)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작\n",
    "start_time = time.time()\n",
    "print(\"=\" * 60)\n",
    "print(f\"학습 시작 - 총 {cfg.EPOCHS} 에폭\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(f\"\\n======== 에폭 {epoch + 1} / {cfg.EPOCHS} ========\")\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # 학습\n",
    "    train_loss, train_grad_norm = train_epoch(model, train_loader, optimizer, scheduler, cfg.DEVICE)\n",
    "\n",
    "    # 검증\n",
    "    val_acc, val_loss = evaluate(model, val_loader, cfg.DEVICE)\n",
    "\n",
    "    # 에폭 종료 시간\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"학습 손실: {train_loss:.4f}\")\n",
    "    print(f\"검증 손실: {val_loss:.4f}\")\n",
    "    print(f\"검증 정확도: {val_acc:.4f}\")\n",
    "    print(f\"그래디언트 노름: {train_grad_norm:.4f}\")\n",
    "    print(f\"에폭 실행 시간: {epoch_time:.2f}초\")\n",
    "\n",
    "# 전체 학습 시간 계산\n",
    "total_time = time.time() - start_time\n",
    "hours = int(total_time // 3600)\n",
    "minutes = int((total_time % 3600) // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "print(\"\\n학습 완료!\")\n",
    "print(f\"전체 학습 시간: {hours:02d}:{minutes:02d}:{seconds:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799426f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "output_dir = f\"./project/sentence/{version}\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"모델이 '{output_dir}' 디렉토리에 저장되었습니다.\")\n",
    "\n",
    "# 예측값 저장\n",
    "val_acc, val_loss, val_predictions, val_labels = evaluate(model, val_loader, cfg.DEVICE, return_predictions=True)\n",
    "\n",
    "np.save(f'{output_dir}/val_predictions.npy', val_predictions)\n",
    "np.save(f'{output_dir}/val_labels.npy', val_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
